

TrainingArguments:
  num_train_epochs: 1  # keep as is for faster training
  warmup_steps: 100  # reduced to match small dataset size
  per_device_train_batch_size: 1  # increased for better CPU utilization
  weight_decay: 0.01  # kept same, but can be reduced if needed
  logging_steps: 50  # less frequent logging
  eval_strategy: steps  # keep the evaluation strategy
  eval_steps: 200  # more frequent evaluation
  save_steps: 10000  # infrequent saving to reduce overhead
  gradient_accumulation_steps: 4  # reduced for faster step processing
